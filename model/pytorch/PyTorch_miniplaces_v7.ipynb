{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models \n",
    "\n",
    "import DataLoader\n",
    "from densenet_modified import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer parameters\n",
    "print_freq_epochs = 100\n",
    "use_cuda = True\n",
    "\n",
    "# Dataset Parameters\n",
    "batch_size = 50\n",
    "load_size = 256\n",
    "fine_size = 224\n",
    "c = 3\n",
    "data_mean = np.asarray([0.45834960097,0.44674252445,0.41352266842])\n",
    "\n",
    "# Training parameters\n",
    "# architecture = 'resnet34'\n",
    "# architecture = 'vgg16_bn'\n",
    "# architecture = 'dense'\n",
    "lr = 0.1  # densenet default = 0.1, \n",
    "lr_init = 0.1\n",
    "momentum = 0.90 # densenet default = 0.9 \n",
    "weight_decay = 1e-3 # densenet default = 1e-4\n",
    "num_epochs = 95\n",
    "\n",
    "dummy_text_file = open(\"dummy_text.txt\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dataloader_disk():\n",
    "    # Construct DataLoader\n",
    "    opt_data_train = {\n",
    "        #'data_h5': 'miniplaces_128_train.h5',\n",
    "        'data_root': '../../data/images/',   # MODIFY PATH ACCORDINGLY\n",
    "        'data_list': '../../data/train.txt', # MODIFY PATH ACCORDINGLY\n",
    "        'load_size': load_size,\n",
    "        'fine_size': fine_size,\n",
    "        'data_mean': data_mean,\n",
    "        'randomize': True\n",
    "        }\n",
    "    opt_data_val = {\n",
    "        #'data_h5': 'miniplaces_128_val.h5',\n",
    "        'data_root': '../../data/images/',   # MODIFY PATH ACCORDINGLY\n",
    "        'data_list': '../../data/val.txt',   # MODIFY PATH ACCORDINGLY\n",
    "        'load_size': load_size,\n",
    "        'fine_size': fine_size,\n",
    "        'data_mean': data_mean,\n",
    "        'randomize': False\n",
    "        }\n",
    "\n",
    "    loader_train = DataLoader.DataLoaderDisk(**opt_data_train)\n",
    "    loader_val = DataLoader.DataLoaderDisk(**opt_data_val)\n",
    "    \n",
    "    return (loader_train, loader_val)\n",
    "\n",
    "def construct_dataloader_disk_128():\n",
    "    # Construct DataLoader\n",
    "    opt_data_train = {\n",
    "        #'data_h5': 'miniplaces_128_train.h5',\n",
    "        'data_root': '../../data/images/',   # MODIFY PATH ACCORDINGLY\n",
    "        'data_list': '../../data/train.txt', # MODIFY PATH ACCORDINGLY\n",
    "        'load_size': 128,\n",
    "        'fine_size': 128,\n",
    "        'data_mean': data_mean,\n",
    "        'randomize': True\n",
    "        }\n",
    "    opt_data_val = {\n",
    "        #'data_h5': 'miniplaces_128_val.h5',\n",
    "        'data_root': '../../data/images/',   # MODIFY PATH ACCORDINGLY\n",
    "        'data_list': '../../data/val.txt',   # MODIFY PATH ACCORDINGLY\n",
    "        'load_size': 128,\n",
    "        'fine_size': 128,\n",
    "        'data_mean': data_mean,\n",
    "        'randomize': False\n",
    "        }\n",
    "\n",
    "    loader_train = DataLoader.DataLoaderDisk(**opt_data_train)\n",
    "    loader_val = DataLoader.DataLoaderDisk(**opt_data_val)\n",
    "    \n",
    "    return (loader_train, loader_val)\n",
    "\n",
    "def construct_dataloader_disk_trainval():\n",
    "    opt_data_trainval = {\n",
    "        #'data_h5': 'miniplaces_128_val.h5',\n",
    "        'data_root': '../../data/images/',   # MODIFY PATH ACCORDINGLY\n",
    "        'data_list': '../../data/trainval.txt',   # MODIFY PATH ACCORDINGLY\n",
    "        'load_size': load_size,\n",
    "        'fine_size': fine_size,\n",
    "        'data_mean': data_mean,\n",
    "        'randomize': False\n",
    "        }\n",
    "    loader_valtrain = DataLoader.DataLoaderDisk(**opt_data_trainval)\n",
    "        \n",
    "    return (loader_valtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def adjust_learning_rate(lr, optimizer, epoch):\n",
    "    \"\"\"Calculates a learning rate of the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = lr_init * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "# def adjust_learning_rate(lr, optimizer, epoch): # for densenet (201)\n",
    "#     \"\"\"Sets the learning rate to the initial LR decayed by 10 after 150 and 225 epochs\"\"\"\n",
    "#     lr = lr_init * (0.1 ** (epoch // 20)) * (0.1 ** (epoch // 50))\n",
    "#     for param_group in optimizer.param_groups:\n",
    "#         param_group['lr'] = lr\n",
    "#     return lr\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(filename, model, state, is_best, epoch):\n",
    "    torch.save(state, \"models/\"+filename) #\"densenet121__retraining.tar\"\n",
    "    if is_best:\n",
    "        torch.save(model, \"results/\"+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and validate methods adapted from https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch, text_file):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i in range(int(train_loader.size()/batch_size)):\n",
    "        input, target = train_loader.next_batch(batch_size)\n",
    "        target = target.long()\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            target = target.cuda(async=True)\n",
    "            input = input.cuda(async=True)\n",
    "        input_var = torch.autograd.Variable(input)\n",
    "        target_var = torch.autograd.Variable(target)\n",
    "        target_var = target_var.long()\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        top1.update(prec1[0], input.size(0))\n",
    "        top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "                \n",
    "        if i % print_freq_epochs == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   epoch, i, train_loader.size()/batch_size, batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1, top5=top5))\n",
    "            \n",
    "    text_file.write(str(epoch)+str(\",\")+str(i)+str(\",\")+str(batch_time.val)+str(\",\")+str(data_time.val)+str(\",\")+str(losses.avg)+str(\",\")+str(top1.avg)+str(\",\")+str(top5.avg)+\"\\n\")\n",
    "        \n",
    "def validate(val_loader, model, criterion, text_file, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i in range(int(val_loader.size()/batch_size)):\n",
    "        input, target = val_loader.next_batch(batch_size)\n",
    "        target = target.long()\n",
    "        if use_cuda:\n",
    "            target = target.cuda(async=True)\n",
    "            input = input.cuda(async=True)\n",
    "        input_var = torch.autograd.Variable(input, volatile=True)\n",
    "        target_var = torch.autograd.Variable(target, volatile=True)\n",
    "        target_var = target_var.long()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        top1.update(prec1[0], input.size(0))\n",
    "        top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        \n",
    "        if i % print_freq_epochs == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   i, val_loader.size()/batch_size, batch_time=batch_time, loss=losses,\n",
    "                   top1=top1, top5=top5))\n",
    "\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'\n",
    "          .format(top1=top1, top5=top5))\n",
    "    \n",
    "    text_file.write(str(\"val,\")+str(epoch)+\",\"+str(i)+str(\",\")+str(batch_time.val)+str(\",\")+str(losses.avg)+str(\",\")+str(top1.avg)+str(\",\")+str(top5.avg)+\"\\n\")\n",
    "\n",
    "    return top5.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if use_cuda:\n",
    "    criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = construct_dataloader_disk()\n",
    "train_loader_128, val_loader_128 = construct_dataloader_disk_128()\n",
    "trainval_loader = construct_dataloader_disk_trainval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DenseNet 121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('results/resnet34.pt')\n",
    "\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_prec5 = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"models/densenet121.pt\")['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"resnet34_adam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_file_train = open(\"results/\"+filename+\".txt\", \"w\")\n",
    "text_file_val = open(\"results/\"+filename+\".txt\", \"w\")\n",
    "\n",
    "for epoch in range(0,num_epochs):\n",
    "#     lr = adjust_learning_rate(lr, optimizer, epoch) # turn off for Adam\n",
    "    print(\"learning rate:\", lr)\n",
    "    \n",
    "    # train for one epoch\n",
    "    train(train_loader, model, criterion, optimizer, epoch, text_file_train)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    prec5 = validate(val_loader, model, criterion, text_file_val, epoch)\n",
    "    \n",
    "    # remember best prec@1 and save checkpoint\n",
    "    is_best = prec5 > best_prec5\n",
    "    best_prec5 = max(prec5, best_prec5)\n",
    "    \n",
    "    save_checkpoint(filename+\".pt\", model, {\n",
    "        'epoch': epoch + 1,\n",
    "        'arch': filename,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec5': best_prec5,\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "    }, is_best, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load(\"results/densenet121.pt\")\n",
    "validate(val_loader_128, dense_model2, criterion, dummy_text_file, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_prec5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate ensable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0x = torch.load('../pytorch/results/densenet169__retraining.pt') # train: * Prec@1 65.902 Prec@5 90.538 ,val: Prec@1 49.070 Prec@5 78.260\n",
    "model1x = torch.load('../pytorch/results/densenet121__retraining.pt') # train * Prec@1 69.012 Prec@5 92.059, val  * Prec@1 51.270 Prec@5 80.680\n",
    "model4x = torch.load('../pytorch/results/best_dense161_retraining.pt') #  * Prec@1 62.776 Prec@5 89.279 val:\" * Prec@1 44.600 Prec@5 75.260\n",
    "dense_model = torch.load('../pytorch/models/best_dense.pt') #   * Prec@1 76.392 Prec@5 95.001 *val Prec@1 51.660 Prec@5 80.690\n",
    "dense_model2 = torch.load('../pytorch/models/best_dense201.pt') #  * train Prec@1 93.920 Prec@5 99.380 val: * Prec@1 51.010 Prec@5 79.500\n",
    "\n",
    "model0 = torch.load('../pytorch_v3/results/resnet34.pt') # 78.92\n",
    "model1 = torch.load('../pytorch_v3/results/resnet50.pt') # 75.4\n",
    "model2 = torch.load('../pytorch_v3/results/resnet101.pt') # 75.39\n",
    "model3 = torch.load('../pytorch_v3/results/resnet152.pt') #  70.4\n",
    "\n",
    "model4 = torch.load('../pytorch_v3/results/resnet34_valtrained.pt') # 77.14?\n",
    "model5 = torch.load('../pytorch_v3/results/resnet50_valtrained.pt') # 75.27?\n",
    "model6 = torch.load('../pytorch_v3/results/resnet101_valtrained.pt') # not trained on val\n",
    "model7 = torch.load('../pytorch_v3/results/resnet152_valtrained.pt') # 70.54\n",
    "\n",
    "models = [model0, model1]\n",
    "models_128 = [model0x,model1x, model4x, dense_model, dense_model2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(val_loader, model0, criterion, dummy_text_file, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dataloader_test():\n",
    "    # Construct DataLoader\n",
    "    opt_data_test = {\n",
    "        'data_root': '../../data/images/',\n",
    "        'load_size': load_size,\n",
    "        'fine_size': fine_size,\n",
    "        'data_mean': data_mean,\n",
    "        'randomize': False # random crops\n",
    "    }\n",
    "    \n",
    "    return DataLoader.DataLoaderDiskTest(**opt_data_test)\n",
    "\n",
    "def construct_dataloader_test_128():\n",
    "    # Construct DataLoader\n",
    "    opt_data_test = {\n",
    "        'data_root': '../../data/images/',\n",
    "        'load_size': 128,\n",
    "        'fine_size': 128,\n",
    "        'data_mean': data_mean,\n",
    "        'randomize': False # random crops\n",
    "    }\n",
    "    \n",
    "    return DataLoader.DataLoaderDiskTest(**opt_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, test_loader_128, models, models_128):\n",
    "    # switch to evaluate mode\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        \n",
    "    for model in models_128:\n",
    "        model.eval()\n",
    "\n",
    "    preds = []\n",
    "    batch_size = 200\n",
    "    crop_iterations = 10\n",
    "    for i in range(int(test_loader.size() / batch_size * crop_iterations)):\n",
    "        input, paths = test_loader.next_batch(batch_size)\n",
    "        input_128, paths = test_loader_128.next_batch(batch_size)\n",
    "        if use_cuda:\n",
    "            input = input.cuda(async=True)\n",
    "            input_128 = input_128.cuda(async=True)\n",
    "                    \n",
    "        input_var = torch.autograd.Variable(input, volatile=True)\n",
    "        input_var_128 = torch.autograd.Variable(input_128, volatile=True)\n",
    "\n",
    "        # compute output\n",
    "        output=0\n",
    "        for model in models:\n",
    "            output += model(input_var)[:,:100]\n",
    "        for model_128 in models_128:\n",
    "            output += model_128(input_var_128)[:,:100]\n",
    "            \n",
    "        preds.append((paths, output))\n",
    "\n",
    "        print('Test: [{0}/{1}]\\t'.format((i+1)*batch_size, test_loader.size()*crop_iterations))\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_ensamble(val_loader, val_loader_128, models, models128, criterion):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "#     losses_128 = AverageMeter()\n",
    "#     top1_128 = AverageMeter()\n",
    "#     top5_128 = AverageMeter()\n",
    "    # switch to evaluate mode\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        \n",
    "    for model in models_128:\n",
    "        model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i in range(int(val_loader.size()/batch_size)):\n",
    "        input, target = val_loader.next_batch(batch_size)\n",
    "        input_128, target_128 = val_loader_128.next_batch(batch_size)\n",
    "        target = target.long()\n",
    "        target_128 = target_128.long()\n",
    "        if use_cuda:\n",
    "            target = target.cuda(async=True)\n",
    "            input = input.cuda(async=True)\n",
    "            target_128 = target_128.cuda(async=True)\n",
    "            input_128 = input_128.cuda(async=True)\n",
    "            \n",
    "        input_var = torch.autograd.Variable(input, volatile=True)\n",
    "        target_var = torch.autograd.Variable(target, volatile=True)\n",
    "        target_var = target_var.long()\n",
    "        \n",
    "        input_var_128 = torch.autograd.Variable(input_128, volatile=True)\n",
    "        target_var_128 = torch.autograd.Variable(target_128, volatile=True)\n",
    "        target_var_128 = target_var_128.long()\n",
    "\n",
    "        # compute output\n",
    "        output=0\n",
    "#         output_128 = 0\n",
    "        for model in models:\n",
    "            output += model(input_var)[:,:100]\n",
    "            \n",
    "        for model_128 in models_128:\n",
    "            output += model_128(input_var_128)[:,:100]\n",
    "\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        top1.update(prec1[0], input.size(0))\n",
    "        top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "                \n",
    "        if i % print_freq_epochs == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   i, val_loader.size()/batch_size, batch_time=batch_time, loss=losses,\n",
    "                   top1=top1, top5=top5))\n",
    "            \n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'\n",
    "          .format(top1=top1, top5=top5))\n",
    "\n",
    "    return top5.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_ensamble(val_loader, val_loader_128, models, models_128, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = construct_dataloader_test()\n",
    "test_loader_128 = construct_dataloader_test_128()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = test(test_loader, test_loader_128, models, models_128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_probs_processed = []\n",
    "diff = 50\n",
    "for i in range(int(len(probs)/10)):\n",
    "    paths = probs[i][0]\n",
    "    output = probs[i][1]\n",
    "    for j in range(1,10):\n",
    "        print(i+j*diff)\n",
    "        output += probs[i+j*diff][1]\n",
    "    batch_probs_processed.append((paths, output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('preds_fixed_vF.txt', 'w') as f:\n",
    "    for paths, batch_probs in batch_probs_processed:\n",
    "        batch_preds = batch_probs.data.topk(5, 1, True, True)[1]\n",
    "        for path, top5 in zip(paths, batch_preds):\n",
    "            path = 'test/' + os.path.basename(path)\n",
    "            f.write('{} {} {} {} {} {}\\n'.format(path, *top5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

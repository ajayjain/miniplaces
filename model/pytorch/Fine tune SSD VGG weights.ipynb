{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train vgg on our dataset (in a separete notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "\n",
    "sys.path.append(\"../../ssd/ssd.pytorch/\")\n",
    "\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.misc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "# from ssd import vgg\n",
    "\n",
    "# from importlib import reload\n",
    "# reload(densenet_modified_vgg)\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Trainer parameters\n",
    "print_freq_epochs = 5\n",
    "use_cuda = True\n",
    "\n",
    "# Dataset Parameters\n",
    "batch_size = 50\n",
    "# load_size = 342\n",
    "# fine_size = 300\n",
    "load_size = 300\n",
    "fine_size = 300\n",
    "c = 3\n",
    "# data_mean = np.asarray([0.45834960097,0.44674252445,0.41352266842])\n",
    "data_mean = np.asarray([0.0, 0.0, 0.0])\n",
    "num_classes = 100\n",
    "\n",
    "# Training parameters\n",
    "# architecture = 'resnet34'\n",
    "# architecture = 'dense'\n",
    "architecture = 'vggbn'\n",
    "lr = 0.0005  # densenet default = 0.1, \n",
    "lr_init = 0.0005\n",
    "momentum = 0.9 # densenet default = 0.9 \n",
    "weight_decay = 1e-3 # densenet default = 1e-4\n",
    "num_epochs = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading data from disk\n",
    "class DataLoaderDisk(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.load_size = int(kwargs['load_size'])\n",
    "        self.fine_size = int(kwargs['fine_size'])\n",
    "        self.data_mean = np.array(kwargs['data_mean'])\n",
    "        self.randomize = kwargs['randomize']\n",
    "        self.data_root = os.path.join(kwargs['data_root'])\n",
    "\n",
    "        # read data info from lists\n",
    "        self.list_im = []\n",
    "        self.list_lab = []\n",
    "        with open(kwargs['data_list'], 'r') as f:\n",
    "            for line in f:\n",
    "                path, lab =line.rstrip().split(' ')\n",
    "                self.list_im.append(os.path.join(self.data_root, path))\n",
    "                self.list_lab.append(int(lab))\n",
    "        self.list_im = np.array(self.list_im, np.object)\n",
    "        self.list_lab = np.array(self.list_lab, np.int64)\n",
    "        self.num = self.list_im.shape[0]\n",
    "        print('# Images found:', self.num)\n",
    "\n",
    "        # permutation\n",
    "        perm = np.random.permutation(self.num) \n",
    "        self.list_im[:, ...] = self.list_im[perm, ...]\n",
    "        self.list_lab[:] = self.list_lab[perm, ...]\n",
    "\n",
    "        self._idx = 0\n",
    "        \n",
    "    def next_batch(self, batch_size):\n",
    "        images_batch = np.zeros((batch_size, self.fine_size, self.fine_size, 3), )\n",
    "        labels_batch = np.zeros(batch_size, dtype=np.double)\n",
    "        for i in range(batch_size):\n",
    "            image = scipy.misc.imread(self.list_im[self._idx])\n",
    "            image = scipy.misc.imresize(image, (self.load_size, self.load_size))\n",
    "            image = image.astype(np.float32)/255.\n",
    "            image = image - self.data_mean\n",
    "            if self.randomize:\n",
    "                flip = np.random.random_integers(0, 1)\n",
    "                if flip>0:\n",
    "                    image = image[:,::-1,:]\n",
    "                offset_h = np.random.random_integers(0, self.load_size-self.fine_size)\n",
    "                offset_w = np.random.random_integers(0, self.load_size-self.fine_size)\n",
    "            else:\n",
    "                offset_h = (self.load_size-self.fine_size)//2\n",
    "                offset_w = (self.load_size-self.fine_size)//2\n",
    "\n",
    "            images_batch[i, ...] =  image[offset_h:offset_h+self.fine_size, offset_w:offset_w+self.fine_size, :]\n",
    "            labels_batch[i, ...] = self.list_lab[self._idx]\n",
    "            \n",
    "            self._idx += 1\n",
    "            if self._idx == self.num:\n",
    "                self._idx = 0\n",
    "        \n",
    "        # Switch to NCHW ordering and convert to torch FloatTensor\n",
    "        images_batch = torch.from_numpy(images_batch.swapaxes(2, 3).swapaxes(1, 2)).float()\n",
    "        labels_batch = torch.from_numpy(labels_batch).long()\n",
    "        return images_batch, labels_batch\n",
    "    \n",
    "    def size(self):\n",
    "        return self.num\n",
    "\n",
    "    def reset(self):\n",
    "        self._idx = 0\n",
    "        \n",
    "def construct_dataloader_disk():\n",
    "    # Construct DataLoader\n",
    "    opt_data_train = {\n",
    "        #'data_h5': 'miniplaces_128_train.h5',\n",
    "        'data_root': '../../data/images/',   # MODIFY PATH ACCORDINGLY\n",
    "        'data_list': '../../data/train.txt', # MODIFY PATH ACCORDINGLY\n",
    "        'load_size': load_size,\n",
    "        'fine_size': fine_size,\n",
    "        'data_mean': data_mean,\n",
    "        'randomize': True\n",
    "        }\n",
    "    opt_data_val = {\n",
    "        #'data_h5': 'miniplaces_128_val.h5',\n",
    "        'data_root': '../../data/images/',   # MODIFY PATH ACCORDINGLY\n",
    "        'data_list': '../../data/val.txt',   # MODIFY PATH ACCORDINGLY\n",
    "        'load_size': load_size,\n",
    "        'fine_size': fine_size,\n",
    "        'data_mean': data_mean,\n",
    "        'randomize': False\n",
    "        }\n",
    "\n",
    "    loader_train = DataLoaderDisk(**opt_data_train)\n",
    "    loader_val = DataLoaderDisk(**opt_data_val)\n",
    "\n",
    "    return (loader_train, loader_val)\n",
    "\n",
    "def construct_dataloader_disk_trainval():\n",
    "    opt_data_trainval = {\n",
    "        #'data_h5': 'miniplaces_128_val.h5',\n",
    "        'data_root': '../../data/images/',   # MODIFY PATH ACCORDINGLY\n",
    "        'data_list': '../../data/trainval.txt',   # MODIFY PATH ACCORDINGLY\n",
    "        'load_size': load_size,\n",
    "        'fine_size': fine_size,\n",
    "        'data_mean': data_mean,\n",
    "        'randomize': False\n",
    "        }\n",
    "    loader_valtrain = DataLoaderDisk(**opt_data_trainval)\n",
    "\n",
    "    return (loader_valtrain)\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def adjust_learning_rate(lr, optimizer, epoch):\n",
    "    \"\"\"Calculates a learning rate of the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = lr_init * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "def save_checkpoint(filename, model, state, is_best, epoch):\n",
    "    torch.save(state, \"models/\"+filename) #\"densenet121__retraining.tar\"\n",
    "    if is_best:\n",
    "        torch.save(model, \"results/\"+filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and load VGG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self, base_vgg_layers, num_classes=100):\n",
    "        super(VGG, self).__init__()\n",
    "        self.base_vgg = base_vgg\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(1024, 512, kernel_size=(3, 3), stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=3, padding=1),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(7 * 7 * 512, 4096),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def features(self, x):\n",
    "        for layer in self.base_vgg:\n",
    "            x = layer(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.layers:\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                print(\"Initializing\", m)\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "\n",
    "#         m = self.classifier[0]\n",
    "#         n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "#         m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "#         if m.bias is not None:\n",
    "#             m.bias.data.zero_()\n",
    "            \n",
    "        for m in self.modules():\n",
    "#             if isinstance(m, nn.Conv2d):\n",
    "#                 n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "#                 m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "#                 if m.bias is not None:\n",
    "#                     m.bias.data.zero_()\n",
    "#             el\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                n = m.weight.size(1)\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "# From ssd.py\n",
    "# This function is derived from torchvision VGG make_layers()\n",
    "# https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\n",
    "def vgg(cfg, in_channels, batch_norm=False):\n",
    "    layers = []\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        elif v == 'C':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "    conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n",
    "    conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n",
    "    layers += [pool5, conv6,\n",
    "               nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)]\n",
    "    return layers\n",
    "\n",
    "vgg_base_configuration = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C', 512, 512, 512, 'M', 512, 512, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ssd_vgg_weights = {}\n",
    "for key, weights in torch.load(\"../../ssd/ssd.pytorch/weights/ssd300_MiniPlaces_0.pth\").items():\n",
    "    # Find weights with keys prefixed by \"vgg.\", and remove prefix\n",
    "    if key[:4] == 'vgg.':\n",
    "        ssd_vgg_weights[key[4:]] = weights\n",
    "\n",
    "base_vgg = nn.ModuleList(vgg(vgg_base_configuration, in_channels=3))\n",
    "base_vgg.load_state_dict(ssd_vgg_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "model = VGG(base_vgg, num_classes=num_classes)\n",
    "\n",
    "if use_cuda:\n",
    "    model = model.cuda(device_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG (\n",
       "  (base_vgg): ModuleList (\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU (inplace)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU (inplace)\n",
       "    (4): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU (inplace)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU (inplace)\n",
       "    (9): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU (inplace)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU (inplace)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU (inplace)\n",
       "    (16): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU (inplace)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU (inplace)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU (inplace)\n",
       "    (23): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU (inplace)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU (inplace)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU (inplace)\n",
       "    (30): MaxPool2d (size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1))\n",
       "    (31): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
       "    (32): ReLU (inplace)\n",
       "    (33): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (34): ReLU (inplace)\n",
       "  )\n",
       "  (layers): Sequential (\n",
       "    (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (2): LeakyReLU (0.01, inplace)\n",
       "    (3): MaxPool2d (size=(3, 3), stride=(3, 3), padding=(1, 1), dilation=(1, 1))\n",
       "  )\n",
       "  (classifier): Sequential (\n",
       "    (0): Linear (25088 -> 4096)\n",
       "    (1): LeakyReLU (0.01, inplace)\n",
       "    (2): Dropout (p = 0.4)\n",
       "    (3): Linear (4096 -> 4096)\n",
       "    (4): LeakyReLU (0.01, inplace)\n",
       "    (5): Dropout (p = 0.4)\n",
       "    (6): Linear (4096 -> 100)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train and validate methods adapted from https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch, text_file):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i in range(int(train_loader.size()/batch_size)):\n",
    "        input, target = train_loader.next_batch(batch_size)\n",
    "        target = target.long()\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            target = target.cuda(async=True)\n",
    "            input = input.cuda(async=True)\n",
    "        input_var = torch.autograd.Variable(input)\n",
    "        target_var = torch.autograd.Variable(target)\n",
    "        target_var = target_var.long()\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        top1.update(prec1[0], input.size(0))\n",
    "        top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "                \n",
    "        if i % print_freq_epochs == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   epoch, i, train_loader.size()/batch_size, batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1, top5=top5))\n",
    "            \n",
    "            with open(text_file, \"a+\") as f:\n",
    "                f.write('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})\\n'.format(\n",
    "                   epoch, i, train_loader.size()/batch_size, batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1, top5=top5))\n",
    "            \n",
    "    with open(text_file, \"a+\") as f:\n",
    "        f.write(str(epoch)+str(\",\")+str(i)+str(\",\")+str(batch_time.val)+str(\",\")+str(data_time.val)+str(\",\")+str(losses.avg)+str(\",\")+str(top1.avg)+str(\",\")+str(top5.avg)+\"\\n\")\n",
    "        \n",
    "def validate(val_loader, model, criterion, text_file, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i in range(int(val_loader.size()/batch_size)):\n",
    "        input, target = val_loader.next_batch(batch_size)\n",
    "        target = target.long()\n",
    "        if use_cuda:\n",
    "            target = target.cuda(async=True)\n",
    "            input = input.cuda(async=True)\n",
    "        input_var = torch.autograd.Variable(input, volatile=True)\n",
    "        target_var = torch.autograd.Variable(target, volatile=True)\n",
    "        target_var = target_var.long()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        top1.update(prec1[0], input.size(0))\n",
    "        top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        \n",
    "        if i % print_freq_epochs == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   i, val_loader.size()/batch_size, batch_time=batch_time, loss=losses,\n",
    "                   top1=top1, top5=top5))\n",
    "\n",
    "            with open(text_file, \"a+\") as f:\n",
    "                f.write('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   i, val_loader.size()/batch_size, batch_time=batch_time, loss=losses,\n",
    "                   top1=top1, top5=top5))\n",
    "\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'\n",
    "          .format(top1=top1, top5=top5))\n",
    "    \n",
    "    with open(text_file, \"a+\") as f:\n",
    "        f.write(str(\"val,\")+str(epoch)+\",\"+str(i)+str(\",\")+str(batch_time.val)+str(\",\")+str(losses.avg)+str(\",\")+str(top1.avg)+str(\",\")+str(top5.avg)+\"\\n\")\n",
    "\n",
    "    return top5.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if use_cuda:\n",
    "    criterion = criterion.cuda(device_id=0)\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Images found: 100000\n",
      "# Images found: 10000\n",
      "# Images found: 110000\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = construct_dataloader_disk()\n",
    "trainval_loader = construct_dataloader_disk_trainval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input, target = train_loader.next_batch(batch_size)\n",
    "# plt.imshow(input[4,:,:,:].permute(1,2,0).numpy() + data_mean)\n",
    "# print((\n",
    "#         (\n",
    "#             (input.sum(2) / 300)\\\n",
    "#                 .sum(2) / 300\n",
    "#         ).sum(0) / 50\n",
    "#     ).numpy())\n",
    "# print(data_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# input, target = train_loader.next_batch(batch_size)\n",
    "\n",
    "# input.size(), target.size()\n",
    "# if use_cuda:\n",
    "#     target = target.cuda(async=True)\n",
    "#     input = input.cuda(async=True)\n",
    "# input_var = torch.autograd.Variable(input)\n",
    "# target_var = torch.autograd.Variable(target)\n",
    "# target_var = target_var.long()\n",
    "\n",
    "# features = model.features(input_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# output = model(input_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_prec5 = 0.0\n",
    "\n",
    "filename = \"vgg_ssd_lr5E-4_bn\"\n",
    "text_file_train = \"results/\"+filename+\".txt\"\n",
    "text_file_val = \"results/\"+filename+\".txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/200.0]\tTime 2.231 (2.231)\tLoss 4.4287 (4.4287)\tPrec@1 0.000 (0.000)\tPrec@5 6.000 (6.000)\n",
      "Test: [5/200.0]\tTime 2.097 (2.116)\tLoss 4.1272 (4.2741)\tPrec@1 4.000 (2.000)\tPrec@5 20.000 (13.667)\n",
      "Test: [10/200.0]\tTime 2.099 (2.108)\tLoss 4.4592 (4.2618)\tPrec@1 2.000 (2.364)\tPrec@5 12.000 (14.545)\n",
      "Test: [15/200.0]\tTime 2.109 (2.107)\tLoss 4.2156 (4.2657)\tPrec@1 6.000 (2.625)\tPrec@5 12.000 (13.875)\n",
      "Test: [20/200.0]\tTime 2.112 (2.107)\tLoss 4.3420 (4.2718)\tPrec@1 4.000 (2.667)\tPrec@5 14.000 (13.810)\n",
      "Test: [25/200.0]\tTime 2.113 (2.108)\tLoss 4.4265 (4.2687)\tPrec@1 2.000 (2.615)\tPrec@5 16.000 (13.846)\n",
      "Test: [30/200.0]\tTime 2.119 (2.109)\tLoss 4.2273 (4.2776)\tPrec@1 4.000 (2.774)\tPrec@5 14.000 (14.000)\n",
      "Test: [35/200.0]\tTime 2.119 (2.111)\tLoss 4.2569 (4.2808)\tPrec@1 4.000 (2.944)\tPrec@5 16.000 (14.056)\n",
      "Test: [40/200.0]\tTime 2.127 (2.112)\tLoss 4.4299 (4.2849)\tPrec@1 0.000 (2.976)\tPrec@5 4.000 (13.902)\n",
      "Test: [45/200.0]\tTime 2.126 (2.114)\tLoss 4.0928 (4.2822)\tPrec@1 2.000 (2.870)\tPrec@5 8.000 (13.652)\n",
      "Test: [50/200.0]\tTime 2.131 (2.115)\tLoss 4.3453 (4.2743)\tPrec@1 0.000 (2.902)\tPrec@5 8.000 (13.725)\n",
      "Test: [55/200.0]\tTime 2.133 (2.116)\tLoss 4.1592 (4.2713)\tPrec@1 8.000 (3.143)\tPrec@5 20.000 (14.214)\n",
      "Test: [60/200.0]\tTime 2.129 (2.117)\tLoss 4.3195 (4.2753)\tPrec@1 2.000 (3.213)\tPrec@5 10.000 (14.262)\n",
      "Test: [65/200.0]\tTime 2.131 (2.118)\tLoss 4.1730 (4.2736)\tPrec@1 4.000 (3.303)\tPrec@5 16.000 (14.242)\n",
      "Test: [70/200.0]\tTime 2.123 (2.119)\tLoss 4.1737 (4.2663)\tPrec@1 6.000 (3.437)\tPrec@5 18.000 (14.563)\n",
      "Test: [75/200.0]\tTime 2.130 (2.119)\tLoss 4.1205 (4.2592)\tPrec@1 6.000 (3.500)\tPrec@5 18.000 (14.842)\n",
      "Test: [80/200.0]\tTime 2.131 (2.120)\tLoss 4.3170 (4.2621)\tPrec@1 2.000 (3.383)\tPrec@5 8.000 (14.741)\n",
      "Test: [85/200.0]\tTime 2.135 (2.121)\tLoss 4.2663 (4.2583)\tPrec@1 2.000 (3.372)\tPrec@5 20.000 (14.953)\n",
      "Test: [90/200.0]\tTime 2.132 (2.121)\tLoss 4.2496 (4.2584)\tPrec@1 2.000 (3.363)\tPrec@5 10.000 (14.901)\n",
      "Test: [95/200.0]\tTime 2.118 (2.122)\tLoss 4.2469 (4.2575)\tPrec@1 2.000 (3.354)\tPrec@5 12.000 (14.896)\n",
      "Test: [100/200.0]\tTime 2.131 (2.122)\tLoss 4.2780 (4.2571)\tPrec@1 4.000 (3.347)\tPrec@5 24.000 (14.950)\n"
     ]
    }
   ],
   "source": [
    "# evaluate on validation set\n",
    "prec5 = validate(val_loader, model, criterion, text_file_val, epoch)\n",
    "\n",
    "# remember best prec@1 and save checkpoint\n",
    "is_best = prec5 > best_prec5\n",
    "best_prec5 = max(prec5, best_prec5)\n",
    "\n",
    "save_checkpoint(filename+\".pt\", model, {\n",
    "    'epoch': epoch + 1,\n",
    "    'arch': filename,\n",
    "    'state_dict': model.state_dict(),\n",
    "    'best_prec5': best_prec5,\n",
    "    'optimizer' : optimizer.state_dict(),\n",
    "}, is_best, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, num_epochs):\n",
    "    # lr = adjust_learning_rate(lr, optimizer, epoch) # turn off for Adam\n",
    "    print(\"learning rate:\", lr)\n",
    "\n",
    "    # train for one epoch\n",
    "    train(train_loader, model, criterion, optimizer, epoch, text_file_train)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    prec5 = validate(val_loader, model, criterion, text_file_val, epoch)\n",
    "\n",
    "    # remember best prec@1 and save checkpoint\n",
    "    is_best = prec5 > best_prec5\n",
    "    best_prec5 = max(prec5, best_prec5)\n",
    "\n",
    "    save_checkpoint(filename+\".pt\", model, {\n",
    "        'epoch': epoch + 1,\n",
    "        'arch': filename,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec5': best_prec5,\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "    }, is_best, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NameError(\"name 'string' is not defined\")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.last_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_vgg = torch.load('results/vgg_ssd.pt')\n",
    "state_latest = torch.load('models/vgg_ssd.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state_latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_dense = torch.load('models/best_dense201.pt') # @Ajay - this loads the pretrained densenet to transfer the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = densenet201() # @Ajay -> this loads densenet_modified_vgg model that is connected to vgg/ssd\n",
    "    \n",
    "if use_cuda:\n",
    "    model = model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if use_cuda:\n",
    "    criterion = criterion.cuda()\n",
    "#     model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False # This prevents training of the densenet parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = 4736\n",
    "num_classes = 100 \n",
    "model.classifier = nn.Linear(num_features, num_classes) #overwrite the last classifier layer\n",
    "\n",
    "# # Parameters of newly constructed modules have requires_grad=True by default\n",
    "# num_ftrs = model_conv.fc.in_features\n",
    "# model_conv.fc = nn.Linear(num_ftrs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay) # full model\n",
    "optimizer = torch.optim.SGD(model.classifier.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay) # last layer\n",
    "\n",
    "# @Ajay - the second optimizer only takes in the parameters from the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_loader, val_loader = construct_dataloader_disk()\n",
    "train_loader, val_loader = construct_dataloader_h5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# need to load weights separately, as the new model has been defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_prec5 = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model 16 epochs on Adam linear retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(0,num_epochs):\n",
    "#     lr = adjust_learning_rate(lr, optimizer, epoch) # off for Adam\n",
    "\n",
    "    text_file = open(\"epoch_output_densenet201_ssd_val_fixed_3.txt\", \"w\")\n",
    "    # train for one epoch\n",
    "    train(train_loader, model, criterion, optimizer, epoch, text_file)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    prec5 = validate(val_loader, model, criterion, text_file)\n",
    "    text_file.close()\n",
    "    \n",
    "    # remember best prec@1 and save checkpoint\n",
    "    is_best = prec5 > best_prec5\n",
    "    best_prec5 = max(prec5, best_prec5)\n",
    "    save_checkpoint(model, {\n",
    "        'epoch': epoch + 1,\n",
    "        'arch': architecture,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec1': best_prec5,\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "    }, is_best, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dummy_text_file = open(\"epoch_outputXX.txt\", \"w\")\n",
    "prec1 = validate(val_loader, model, criterion, dummy_text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# torch.save(model,'densenet_201_ssd_linear_retrained_2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = torch.load(\"densenet_201_ssd_linear_retrained.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_prec5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dense_model = torch.load('models/best_dense.pt')\n",
    "# dense_model2 = torch.load('models/best_dense201.pt')\n",
    "dense_model2 = torch.load('densenet_201_ssd_100p.pt')\n",
    "resnet_model = torch.load('models/best_resnet34.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate_ensamble(val_loader, model1, model2, model3, criterion):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i in range(int(val_loader.size()/batch_size)):\n",
    "        input, target = val_loader.next_batch(batch_size)\n",
    "        target = target.long()\n",
    "        if use_cuda:\n",
    "            target = target.cuda(async=True)\n",
    "            input = input.cuda(async=True)\n",
    "        input_var = torch.autograd.Variable(input, volatile=True)\n",
    "        target_var = torch.autograd.Variable(target, volatile=True)\n",
    "        target_var = target_var.long()\n",
    "\n",
    "        # compute output\n",
    "        output1 = model1(input_var)\n",
    "        output2 = model2(input_var)\n",
    "        output3 = model3(input_var)\n",
    "        output = output3+output1+output2\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        top1.update(prec1[0], input.size(0))\n",
    "        top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "                \n",
    "        if i % print_freq_epochs == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   i, val_loader.size()/batch_size, batch_time=batch_time, loss=losses,\n",
    "                   top1=top1, top5=top5))\n",
    "\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'\n",
    "          .format(top1=top1, top5=top5))\n",
    "\n",
    "    return top5.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validate_ensamble(val_loader, dense_model, dense_model, dense_model2, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(test_loader, model1):\n",
    "    # switch to evaluate mode\n",
    "    model1.eval()\n",
    "\n",
    "    preds = []\n",
    "    \n",
    "    for i in range(int(test_loader.size() / batch_size)):\n",
    "        input, paths = test_loader.next_batch(batch_size)\n",
    "        if use_cuda:\n",
    "            input = input.cuda(async=True)\n",
    "        input_var = torch.autograd.Variable(input, volatile=True)\n",
    "\n",
    "        # compute output\n",
    "        output1 = model1(input_var)\n",
    "        output = output1 \n",
    "        preds.append((paths, output))\n",
    "\n",
    "        print('Test: [{0}/{1}]\\t'.format((i+1)*batch_size, test_loader.size()))\n",
    "\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_dataloader_test():\n",
    "    # Construct DataLoader\n",
    "    opt_data_test = {\n",
    "        'data_root': '../../data/images/',\n",
    "        'load_size': load_size,\n",
    "        'fine_size': fine_size,\n",
    "        'data_mean': data_mean,\n",
    "        'randomize': False\n",
    "    }\n",
    "    \n",
    "    return DataLoader.DataLoaderDiskTest(**opt_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_loader = construct_dataloader_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probs = test(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('_preds_fixed12.txt', 'w') as f:\n",
    "    for paths, batch_probs in probs:\n",
    "        batch_preds = batch_probs.data.topk(5, 1, True, True)[1]\n",
    "        for path, top5 in zip(paths, batch_preds):\n",
    "            path = 'test/' + os.path.basename(path)\n",
    "            f.write('{} {} {} {} {} {}\\n'.format(path, *top5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loader_train, loader_val = construct_dataloader_h5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch = loader_train.next_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import augmentations\n",
    "from sklearn.preprocessing import scale\n",
    "# plt.imshow(loader_train.im_set[0])\n",
    "pd = augmentations.RandomBrightness()\n",
    "X = loader_train.im_set[1]\n",
    "# X = pd(X.astype(np.float32), None, None)[0][:,:,::-1]\n",
    "# X /= (X.max())/255.\n",
    "plt.imshow(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(batch[0][0].permute(1,2,0).numpy()[:,:,:]+.5)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

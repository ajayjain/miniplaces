{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "import DataLoader\n",
    "from densenet_modified import densenet201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer parameters\n",
    "print_freq_epochs = 100\n",
    "use_cuda = True\n",
    "\n",
    "# Dataset Parameters\n",
    "batch_size = 64\n",
    "load_size = 128\n",
    "fine_size = 128\n",
    "c = 3\n",
    "data_mean = np.asarray([0.45834960097,0.44674252445,0.41352266842])\n",
    "\n",
    "# Training parameters\n",
    "# architecture = 'resnet34'\n",
    "architecture = 'dense'\n",
    "lr = 0.1  # Can be large for resnet or if using batch norm\n",
    "momentum = 0.90\n",
    "weight_decay = 1e-4\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dataloader_disk():\n",
    "    # Construct DataLoader\n",
    "    opt_data_train = {\n",
    "        #'data_h5': 'miniplaces_128_train.h5',\n",
    "        'data_root': '../../data/images/',   # MODIFY PATH ACCORDINGLY\n",
    "        'data_list': '../../data/train.txt', # MODIFY PATH ACCORDINGLY\n",
    "        'load_size': load_size,\n",
    "        'fine_size': fine_size,\n",
    "        'data_mean': data_mean,\n",
    "        'randomize': True\n",
    "        }\n",
    "    opt_data_val = {\n",
    "        #'data_h5': 'miniplaces_128_val.h5',\n",
    "        'data_root': '../../data/images/',   # MODIFY PATH ACCORDINGLY\n",
    "        'data_list': '../../data/val.txt',   # MODIFY PATH ACCORDINGLY\n",
    "        'load_size': load_size,\n",
    "        'fine_size': fine_size,\n",
    "        'data_mean': data_mean,\n",
    "        'randomize': False\n",
    "        }\n",
    "\n",
    "    loader_train = DataLoader.DataLoaderDisk(**opt_data_train)\n",
    "    loader_val = DataLoader.DataLoaderDisk(**opt_data_val)\n",
    "    \n",
    "    return (loader_train, loader_val)\n",
    "\n",
    "def construct_dataloader_h5():\n",
    "    # Construct DataLoader from an h5 file\n",
    "    opt_data_train = {\n",
    "        'data_h5': 'miniplaces_128_train.h5',\n",
    "        #'data_root': '../../data/images/',   # MODIFY PATH ACCORDINGLY\n",
    "        #'data_list': '../../data/train.txt', # MODIFY PATH ACCORDINGLY\n",
    "        'load_size': load_size,\n",
    "        'fine_size': fine_size,\n",
    "        'data_mean': data_mean,\n",
    "        'randomize': True\n",
    "        }\n",
    "    opt_data_val = {\n",
    "        'data_h5': 'miniplaces_128_val.h5',\n",
    "        #'data_root': '../../data/images/',   # MODIFY PATH ACCORDINGLY\n",
    "        #'data_list': '../../data/val.txt',   # MODIFY PATH ACCORDINGLY\n",
    "        'load_size': load_size,\n",
    "        'fine_size': fine_size,\n",
    "        'data_mean': data_mean,\n",
    "        'randomize': False\n",
    "        }\n",
    "\n",
    "    loader_train = DataLoader.DataLoaderH5(**opt_data_train)\n",
    "    loader_val = DataLoader.DataLoaderH5(**opt_data_val)\n",
    "    \n",
    "    return (loader_train, loader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def adjust_learning_rate(lr, optimizer, epoch):\n",
    "    \"\"\"Calculates a learning rate of the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "def save_checkpoint(model, state, is_best, epoch):\n",
    "    torch.save(state, \"models/\"+\"checkpoint_dense201.tar\")\n",
    "    if is_best:\n",
    "        torch.save(model, \"models/\"+\"best_dense201.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and validate methods adapted from https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch, text_file):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i in range(int(train_loader.size()/batch_size)):\n",
    "        input, target = train_loader.next_batch(batch_size)\n",
    "        target = target.long()\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            target = target.cuda(async=True)\n",
    "            input = input.cuda(async=True)\n",
    "        input_var = torch.autograd.Variable(input)\n",
    "        target_var = torch.autograd.Variable(target)\n",
    "        target_var = target_var.long()\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        top1.update(prec1[0], input.size(0))\n",
    "        top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        text_file.write(str(epoch)+str(\",\")+str(i)+str(\",\")+str(batch_time.val)+str(\",\")+str(data_time.val)+str(\",\")+str(losses.val)+str(\",\")+str(top1.val)+str(\",\")+str(top5.val)+\"\\n\")\n",
    "        \n",
    "        if i % print_freq_epochs == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   epoch, i, train_loader.size()/batch_size, batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1, top5=top5))\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion, text_file):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i in range(int(val_loader.size()/batch_size)):\n",
    "        input, target = val_loader.next_batch(batch_size)\n",
    "        target = target.long()\n",
    "        if use_cuda:\n",
    "            target = target.cuda(async=True)\n",
    "            input = input.cuda(async=True)\n",
    "        input_var = torch.autograd.Variable(input, volatile=True)\n",
    "        target_var = torch.autograd.Variable(target, volatile=True)\n",
    "        target_var = target_var.long()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        top1.update(prec1[0], input.size(0))\n",
    "        top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        text_file.write(str(\"val, \")+str(i)+str(\",\")+str(batch_time.val)+str(\",\")+str(losses.val)+str(\",\")+str(top1.val)+str(\",\")+str(top5.val)+\"\\n\")\n",
    "        \n",
    "        if i % print_freq_epochs == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   i, val_loader.size()/batch_size, batch_time=batch_time, loss=losses,\n",
    "                   top1=top1, top5=top5))\n",
    "\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'\n",
    "          .format(top1=top1, top5=top5))\n",
    "\n",
    "    return top5.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if architecture[:5]!='dense':\n",
    "    model = models.__dict__[architecture]()\n",
    "else:\n",
    "    model = densenet201()\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if use_cuda:\n",
    "    criterion = criterion.cuda()\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.avgpool = nn.AdaptiveAvgPool2d(1) # to be used for ResNets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Images found: 100000\n",
      "# Images found: 10000\n"
     ]
    }
   ],
   "source": [
    "# train_loader, val_loader = construct_dataloader_disk()\n",
    "train_loader, val_loader = construct_dataloader_h5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load('resnet504-3.0.ph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_prec5 = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/1562.5]\tTime 0.445 (0.445)\tData 0.034 (0.034)\tLoss 6.8916 (6.8916)\tPrec@1 0.000 (0.000)\tPrec@5 1.562 (1.562)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-ce6ba14aa0ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtext_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch_output_densenet201.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# evaluate on validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-a2aa56f1eb4f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, criterion, optimizer, epoch, text_file)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# measure accuracy and record loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mprec1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mtop1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprec1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mtop5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprec5\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1,num_epochs):\n",
    "    lr = adjust_learning_rate(lr, optimizer, epoch)\n",
    "\n",
    "    text_file = open(\"epoch_output_densenet201.txt\", \"w\")\n",
    "    # train for one epoch\n",
    "    train(train_loader, model, criterion, optimizer, epoch, text_file)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    prec5 = validate(val_loader, model, criterion, text_file)\n",
    "    text_file.close()\n",
    "    \n",
    "    # remember best prec@1 and save checkpoint\n",
    "    is_best = prec5 > best_prec5\n",
    "    best_prec5 = max(prec5, best_prec5)\n",
    "    save_checkpoint(model, {\n",
    "        'epoch': epoch + 1,\n",
    "        'arch': architecture,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec1': best_prec5,\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "    }, is_best, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/156.25]\tTime 0.119 (0.119)\tLoss 6.9066 (6.9066)\tPrec@1 1.562 (1.562)\tPrec@5 3.125 (3.125)\n",
      "Test: [100/156.25]\tTime 0.102 (0.102)\tLoss 6.9088 (6.9075)\tPrec@1 0.000 (1.006)\tPrec@5 0.000 (1.083)\n",
      " * Prec@1 0.891 Prec@5 0.992\n"
     ]
    }
   ],
   "source": [
    "dummy_text_file = open(\"epoch_outputXX.txt\", \"w\")\n",
    "prec1 = validate(val_loader, model, criterion, dummy_text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'densenet_ep23_asymptotic.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.66907051282051"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_prec5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_model = torch.load('models/best_dense.pt')\n",
    "dense_model2 = torch.load('models/best_dense201.pt')\n",
    "resnet_model = torch.load('models/best_resnet34.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_ensamble(val_loader, model1, model2, model3, criterion):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i in range(int(val_loader.size()/batch_size)):\n",
    "        input, target = val_loader.next_batch(batch_size)\n",
    "        target = target.long()\n",
    "        if use_cuda:\n",
    "            target = target.cuda(async=True)\n",
    "            input = input.cuda(async=True)\n",
    "        input_var = torch.autograd.Variable(input, volatile=True)\n",
    "        target_var = torch.autograd.Variable(target, volatile=True)\n",
    "        target_var = target_var.long()\n",
    "\n",
    "        # compute output\n",
    "        output1 = model1(input_var)\n",
    "        output2 = model2(input_var)\n",
    "        output3 = model3(input_var)\n",
    "        output = output3+output1+output2\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        top1.update(prec1[0], input.size(0))\n",
    "        top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "                \n",
    "        if i % print_freq_epochs == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   i, val_loader.size()/batch_size, batch_time=batch_time, loss=losses,\n",
    "                   top1=top1, top5=top5))\n",
    "\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'\n",
    "          .format(top1=top1, top5=top5))\n",
    "\n",
    "    return top5.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/156.25]\tTime 0.872 (0.872)\tLoss 2.8430 (2.8430)\tPrec@1 57.812 (57.812)\tPrec@5 85.938 (85.938)\n",
      "Test: [100/156.25]\tTime 0.191 (0.194)\tLoss 3.3035 (3.6776)\tPrec@1 59.375 (54.394)\tPrec@5 85.938 (82.147)\n",
      " * Prec@1 54.137 Prec@5 82.302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "82.3016826923077"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_ensamble(val_loader, dense_model, dense_model, dense_model2, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

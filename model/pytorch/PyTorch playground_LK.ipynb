{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Trainer parameters\n",
    "print_freq_epochs = 1\n",
    "use_cuda = False\n",
    "\n",
    "# Dataset Parameters\n",
    "batch_size = 200\n",
    "load_size = 128\n",
    "fine_size = 128\n",
    "c = 3\n",
    "data_mean = np.asarray([0.45834960097,0.44674252445,0.41352266842])\n",
    "\n",
    "# Training parameters\n",
    "architecture = 'densenet121'\n",
    "lr = 0.005  # Can be large for resnet or if using batch norm\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/lkulik/PycharmProjects/miniplaces/model/pytorch'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_dataloader_disk():\n",
    "    # Construct DataLoader\n",
    "    opt_data_train = {\n",
    "        #'data_h5': 'miniplaces_128_train.h5',\n",
    "        'data_root': '../../data/images/',   # MODIFY PATH ACCORDINGLY\n",
    "        'data_list': '../../data/train.txt', # MODIFY PATH ACCORDINGLY\n",
    "        'load_size': load_size,\n",
    "        'fine_size': fine_size,\n",
    "        'data_mean': data_mean,\n",
    "        'randomize': True\n",
    "        }\n",
    "    opt_data_val = {\n",
    "        #'data_h5': 'miniplaces_128_val.h5',\n",
    "        'data_root': '../../data/images/',   # MODIFY PATH ACCORDINGLY\n",
    "        'data_list': '../../data/val.txt',   # MODIFY PATH ACCORDINGLY\n",
    "        'load_size': load_size,\n",
    "        'fine_size': fine_size,\n",
    "        'data_mean': data_mean,\n",
    "        'randomize': False\n",
    "        }\n",
    "\n",
    "    loader_train = DataLoader.DataLoaderDisk(**opt_data_train)\n",
    "    loader_val = DataLoader.DataLoaderDisk(**opt_data_val)\n",
    "    \n",
    "    return (loader_train, loader_val)\n",
    "\n",
    "def construct_dataloader_h5():\n",
    "    # Construct DataLoader from an h5 file\n",
    "    opt_data_train = {\n",
    "        'data_h5': 'miniplaces_128_train.h5',\n",
    "        #'data_root': '../../data/images/',   # MODIFY PATH ACCORDINGLY\n",
    "        #'data_list': '../../data/train.txt', # MODIFY PATH ACCORDINGLY\n",
    "        'load_size': load_size,\n",
    "        'fine_size': fine_size,\n",
    "        'data_mean': data_mean,\n",
    "        'randomize': True\n",
    "        }\n",
    "    opt_data_val = {\n",
    "        'data_h5': 'miniplaces_128_val.h5',\n",
    "        #'data_root': '../../data/images/',   # MODIFY PATH ACCORDINGLY\n",
    "        #'data_list': '../../data/val.txt',   # MODIFY PATH ACCORDINGLY\n",
    "        'load_size': load_size,\n",
    "        'fine_size': fine_size,\n",
    "        'data_mean': data_mean,\n",
    "        'randomize': False\n",
    "        }\n",
    "\n",
    "    loader_train = DataLoader.DataLoaderH5(**opt_data_train)\n",
    "    loader_val = DataLoader.DataLoaderH5(**opt_data_val)\n",
    "    \n",
    "    return (loader_train, loader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def adjust_learning_rate(lr, optimizer, epoch):\n",
    "    \"\"\"Calculates a learning rate of the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train and validate methods adapted from https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i in range(train_loader.size()):\n",
    "        input, target = train_loader.next_batch(batch_size)\n",
    "        \n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            target = target.cuda(async=True)\n",
    "        input_var = torch.autograd.Variable(input)\n",
    "        target_var = torch.autograd.Variable(target)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        top1.update(prec1[0], input.size(0))\n",
    "        top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % print_freq_epochs == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   epoch, i, train_loader.size(), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1, top5=top5))\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i in range(val_loader.size()):\n",
    "        input, target = val_loader.next_batch(batch_size)\n",
    "        \n",
    "        if use_cuda:\n",
    "            target = target.cuda(async=True)\n",
    "        input_var = torch.autograd.Variable(input, volatile=True)\n",
    "        target_var = torch.autograd.Variable(target, volatile=True)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        top1.update(prec1[0], input.size(0))\n",
    "        top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % print_freq_epochs == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   i, val_loader.size(), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1, top5=top5))\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'\n",
    "          .format(top1=top1, top5=top5))\n",
    "\n",
    "    return top1.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.__dict__[architecture]()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if use_cuda:\n",
    "    criterion = criterion.cuda()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr,\n",
    "                            momentum=momentum,\n",
    "                            weight_decay=weight_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Images found: 100000\n",
      "# Images found: 10000\n"
     ]
    }
   ],
   "source": [
    "# train_loader, val_loader = construct_dataloader_disk()\n",
    "train_loader, val_loader = construct_dataloader_h5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I probably want to create a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for epoch in range(num_epochs):\n",
    "\n",
    "lr = adjust_learning_rate(lr, optimizer, epoch)\n",
    "\n",
    "# train for one epoch\n",
    "train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "# evaluate on validation set\n",
    "prec1 = validate(val_loader, model, criterion)\n",
    "\n",
    "# remember best prec@1 and save checkpoint\n",
    "is_best = prec1 > best_prec1\n",
    "best_prec1 = max(prec1, best_prec1)\n",
    "save_checkpoint({\n",
    "    'epoch': epoch + 1,\n",
    "    'arch': architecture,\n",
    "    'state_dict': model.state_dict(),\n",
    "    'best_prec1': best_prec1,\n",
    "    'optimizer' : optimizer.state_dict(),\n",
    "}, is_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

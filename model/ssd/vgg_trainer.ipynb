{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the SSD model\n",
    "\n",
    "This is an incomplete notebook for training an SSD model. It's dependent on code in https://github.com/amdegroot/ssd.pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from data import v2, v1, VOCDetection, detection_collate, VOCroot, VOC_CLASSES\n",
    "from utils.augmentations import SSDAugmentation\n",
    "from layers.modules import MultiBoxLoss\n",
    "from ssd import build_ssd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssd_dim = 300\n",
    "# means = (104, 117, 123)  # only support voc now. TODO: Change to miniplaces mean\n",
    "means = (0.45834960097,0.44674252445,0.41352266842)\n",
    "num_classes = 175\n",
    "batch_size = 16\n",
    "accum_batch_size = 32\n",
    "\n",
    "iter_size = accum_batch_size / batch_size\n",
    "start_iter = 0\n",
    "max_iter = 120000\n",
    "\n",
    "transform = SSDAugmentation(ssd_dim, means)  # Change to None to skip data augmentation\n",
    "\n",
    "lr = 1e-3\n",
    "weight_decay = 0.0005\n",
    "stepvalues = (80000, 100000, 120000)\n",
    "gamma = 0.1\n",
    "momentum = 0.9\n",
    "\n",
    "resume = \"\"\n",
    "\n",
    "cuda = True\n",
    "num_workers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier(param):\n",
    "    init.xavier_uniform(param)\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        xavier(m.weight.data)\n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build network and initialize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing weights...\n"
     ]
    }
   ],
   "source": [
    "ssd_net = build_ssd('train', ssd_dim, num_classes)\n",
    "\n",
    "if cuda and torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    net = torch.nn.DataParallel(ssd_net).cuda()\n",
    "    cudnn.benchmark = True\n",
    "else:\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "    net = ssd_net\n",
    "    \n",
    "\n",
    "if resume:\n",
    "    print('Resuming training, loading {}...'.format(resume))\n",
    "    ssd_net.load_weights(resume)\n",
    "else:\n",
    "    # vgg_weights = torch.load(args.save_folder + args.basenet)\n",
    "    # print('Loading base network...')\n",
    "    # ssd_net.vgg.load_state_dict(vgg_weights)\n",
    "    \n",
    "    print('Initializing weights...')\n",
    "    # initialize newly added layers' weights with xavier method\n",
    "    ssd_net.extras.apply(weights_init)\n",
    "    ssd_net.loc.apply(weights_init)\n",
    "    ssd_net.conf.apply(weights_init)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=lr,\n",
    "                      momentum=momentum, weight_decay=weight_decay)\n",
    "criterion = MultiBoxLoss(num_classes, 0.5, True, 0, True, 3, 0.5, False, cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniPlacesDetection(data.Dataset):\n",
    "    \"\"\"MiniPlaces object detection object\n",
    "    \n",
    "    input is image, target is annotation\n",
    "    \n",
    "    Arguments:\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root=\"\", mode=\"train\", transform=None, dataset_name='MiniPlaces'):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.name = dataset_name\n",
    "        self.mode = mode\n",
    "        self.xml_glob_pattern = os.path.join(self.root, \"objects\", self.mode, \"*/*/*.xml\")\n",
    "        \n",
    "        self.load_objects()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.objects)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        im, target, h, w = self.pull_item(index)\n",
    "        return im, target\n",
    "\n",
    "    def pull_image(self, index):\n",
    "        '''Returns the original image object at index in PIL form\n",
    "\n",
    "        Note: not using self.__getitem__(), as any transformations passed in\n",
    "        could mess up this functionality.\n",
    "\n",
    "        Argument:\n",
    "            index (int): index of img to show\n",
    "        Return:\n",
    "            PIL img\n",
    "        '''\n",
    "        obj = self.objects[index]\n",
    "        path = os.path.join(self.root, obj[\"path\"])\n",
    "        im = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "        if im.shape[1] != ssd_dim or im.shape[2] != ssd_dim:\n",
    "            im = cv2.resize(im, (ssd_dim, ssd_dim))\n",
    "        return im\n",
    "\n",
    "    def pull_item(self, index):\n",
    "        img = self.pull_image(index)\n",
    "        height, width, channels = img.shape\n",
    "        \n",
    "        obj = self.objects[index]\n",
    "        target = []\n",
    "        for i, objectdata in enumerate(obj[\"objects\"]):\n",
    "            box_and_label = objectdata[:]\n",
    "            box_and_label[0] /= 128\n",
    "            box_and_label[1] /= 128\n",
    "            box_and_label[2] /= 128\n",
    "            box_and_label[3] /= 128\n",
    "            target.append(box_and_label)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            target = np.array(target)\n",
    "            \n",
    "            img, boxes, labels = self.transform(img, target[:, :4], target[:, 4])\n",
    "            # to rgb\n",
    "            img = img[:, :, (2, 1, 0)]\n",
    "            # img = img.transpose(2, 0, 1)\n",
    "            target = np.hstack((boxes, np.expand_dims(labels, axis=1)))\n",
    "\n",
    "        return torch.from_numpy(img).permute(2, 0, 1).cuda(), target, height, width\n",
    "\n",
    "    def load_objects(self):\n",
    "        object_files = glob.glob(self.xml_glob_pattern)\n",
    "        object_files.sort()\n",
    "        \n",
    "        self.objects = list()\n",
    "\n",
    "        for path in object_files:\n",
    "            with open(path, \"r\") as f:\n",
    "                xml = f.read()\n",
    "                xml = \"\"\"<?xml version=\"1.0\"?>\n",
    "                <base>\n",
    "                \"\"\" + xml + \"\"\"\n",
    "                </base>\n",
    "                \"\"\"\n",
    "\n",
    "                tree = ET.fromstring(xml)\n",
    "                filename = os.path.basename(path).replace(\".xml\", \".jpg\")\n",
    "                path = os.path.join(self.root, \"images\", self.mode, tree.find(\"folder\").text[1:], filename)\n",
    "\n",
    "                objs = []\n",
    "                for obj in tree.findall(\"objects\"):\n",
    "                    bndbox = obj.find(\"bndbox\")\n",
    "                    objdata = [\n",
    "                        int(bndbox.find(\"xmin\").text),\n",
    "                        int(bndbox.find(\"ymin\").text),\n",
    "                        int(bndbox.find(\"xmax\").text),\n",
    "                        int(bndbox.find(\"ymax\").text),\n",
    "                        int(obj.find(\"class\").text),\n",
    "                    ]\n",
    "                    objs.append(objdata)\n",
    "\n",
    "                data = {\n",
    "                    \"path\": path,\n",
    "                    \"class\": int(tree.find(\"class\").text),\n",
    "                    \"objects\": objs,\n",
    "                }\n",
    "                \n",
    "                if len(objs):\n",
    "                    self.objects.append(data)\n",
    "        print(\"Num train images:\", len(self.objects))\n",
    "        print(\"Num bounding boxes:\", sum(map(lambda o: len(o['objects']), self.objects)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num train images: 3296\n",
      "Num bounding boxes: 46706\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "can't convert CUDA tensor to numpy (it doesn't support GPU arrays). Use .cpu() to move the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-cf9e354bd24c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmeans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: can't convert CUDA tensor to numpy (it doesn't support GPU arrays). Use .cpu() to move the tensor to host memory first."
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYfAlqQmDL0lNGHxJauJ/Acz2XLpusNoKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd5d62b8f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = MiniPlacesDetection(\n",
    "            root=\"/workspace/miniplaces/data\",\n",
    "            mode=\"train\",\n",
    "            transform=None)\n",
    "\n",
    "img, target, h, w = dataset.pull_item(50)\n",
    "\n",
    "fig,ax = plt.subplots(1)\n",
    "ax.imshow((img.permute(1, 2, 0).numpy() + means).astype(np.uint8)[:,:,::-1])\n",
    "\n",
    "for i in range(len(target)):\n",
    "    rect = patches.Rectangle(\n",
    "        (target[i][0] * w, target[i][1] * h),\n",
    "        target[i][2] * w - target[i][0] * w,\n",
    "        target[i][3] * h - target[i][1] * h,\n",
    "        linewidth=2,edgecolor='r',facecolor='none')\n",
    "    print(\"Object:\", target[i])\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, gamma, step):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 at every specified step\n",
    "    # Adapted from PyTorch Imagenet example:\n",
    "    # https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "    \"\"\"\n",
    "    lr = lr * (gamma ** (step))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "net.cuda()\n",
    "net.train()\n",
    "criterion.cuda()\n",
    "\n",
    "# loss counters\n",
    "loc_loss = 0  # epoch\n",
    "conf_loss = 0\n",
    "epoch = 0\n",
    "\n",
    "print('Loading Dataset...')\n",
    "dataset = MiniPlacesDetection(\n",
    "            root=\"/workspace/miniplaces/data\",\n",
    "            mode=\"train\",\n",
    "            transform=None)\n",
    "print('Training SSD on', dataset.name)\n",
    "\n",
    "epoch_size = len(dataset) // batch_size\n",
    "step_index = 0\n",
    "batch_iterator = None\n",
    "data_loader = data.DataLoader(dataset, batch_size, num_workers=num_workers,\n",
    "                              shuffle=True, collate_fn=detection_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(start_iter, max_iter):\n",
    "    if (not batch_iterator) or (iteration % epoch_size == 0):\n",
    "        # create batch iterator\n",
    "        batch_iterator = iter(data_loader)\n",
    "\n",
    "    if iteration in stepvalues:\n",
    "        step_index += 1\n",
    "        adjust_learning_rate(optimizer, gamma, step_index)\n",
    "        # reset epoch loss counters\n",
    "        loc_loss = 0\n",
    "        conf_loss = 0\n",
    "        epoch += 1\n",
    "\n",
    "    # load train data\n",
    "    images, targets = next(batch_iterator)\n",
    "    \n",
    "    if cuda:\n",
    "        images = images.float() / 255. - 0.5\n",
    "        images = Variable(images.cuda(async=True))\n",
    "        targets = [Variable(anno.cuda(async=True), volatile=True) for anno in targets]\n",
    "    else:\n",
    "        images = Variable(images.float())\n",
    "        targets = [Variable(anno, volatile=True) for anno in targets]\n",
    "\n",
    "    # forward\n",
    "    t0 = time.time()\n",
    "    out = net(images)\n",
    "    out = tuple(out[i].cuda() for i in range(len(out)))\n",
    "\n",
    "    # backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss_l, loss_c = criterion(out, targets)\n",
    "    loss = loss_l + loss_c\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    t1 = time.time()\n",
    "    \n",
    "    # record loss\n",
    "    loc_loss += loss_l.data[0]\n",
    "    conf_loss += loss_c.data[0]\n",
    "    \n",
    "    # if iteration % 10 == 0:\n",
    "    print('Timer: %.4f sec.' % (t1 - t0))\n",
    "    print('iter ' + repr(iteration) + ' || Loss: %.4f ||' % (loss.data[0]), end=' ')\n",
    "    \n",
    "    if iteration % 100 == 0 or iteration == max_iter - 1:\n",
    "        print('Saving state, iter:', iteration)\n",
    "        torch.save(ssd_net.state_dict(),\n",
    "                   'weights/ssd{}_{}_{}.pth'.format(ssd_dim, dataset.name, repr(iteration)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
